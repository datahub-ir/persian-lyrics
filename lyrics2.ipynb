{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "lyrics2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4oSIEC_0TaJ",
        "outputId": "4e72044a-e197-446f-dcfb-327155e5a5eb"
      },
      "source": [
        "!pip install hazm\n",
        "!pip install summa\n",
        "!pip install persian_wordcloud"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Requirement already satisfied: summa in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Requirement already satisfied: persian_wordcloud in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from persian_wordcloud) (1.5.0)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.7/dist-packages (from persian_wordcloud) (0.4.2)\n",
            "Requirement already satisfied: arabic-reshaper in /usr/local/lib/python3.7/dist-packages (from persian_wordcloud) (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud->persian_wordcloud) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->persian_wordcloud) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-bidi->persian_wordcloud) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper->persian_wordcloud) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper->persian_wordcloud) (54.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAR6B1GJ0KtM"
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re   \n",
        "import hazm\n",
        "from summa import keywords\n",
        "from persian_wordcloud.wordcloud import PersianWordCloud"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g04_uK7V0KtY"
      },
      "source": [
        "lemm = hazm.Lemmatizer()\n",
        "normalizer = hazm.Normalizer()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzHPz0ZH0Kta"
      },
      "source": [
        "def preprocess(df, stopwords):\n",
        "    df['ptext'] = df['text'].apply(lambda x : re.sub(r'[\\s]{2,}',' ',x))\n",
        "    df['ptext'] = df.ptext.apply(lambda x : x.replace(\"\\n\",\" \"))    \n",
        "    df[\"ptext\"] = df.ptext.apply(lambda x : ' '.join([normalizer.normalize(word) for word in hazm.word_tokenize(x)]))    \n",
        "    df['ptext'] = df.ptext.apply(lambda x :' '.join([lemm.lemmatize(word) for word in hazm.word_tokenize(x)]))\n",
        "    df['ptext'] = df['ptext'].apply(lambda x : re.sub('[^\\u0600-\\u06FF\\s]',' ',x))\n",
        "    df['ptext'] = df.ptext.apply(lambda x : ' '.join([word for word in hazm.word_tokenize(x) if word not in stopwords]))\n",
        "    return df"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeJfRprd0Ktc"
      },
      "source": [
        "dic_key = []\n",
        "df = pd.read_excel('lyrics.xlsx')\n",
        "stopwords = pd.read_csv('stopwords.txt',\n",
        "                        sep='\\n',\n",
        "                        quotechar=None,\n",
        "                        quoting=csv.QUOTE_NONE,\n",
        "                        header=None)[0].tolist()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05u3en5B0Ktd"
      },
      "source": [
        "df = preprocess(df, stopwords)\n",
        "array_text = df.ptext.values\n",
        "kewords_list=[]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L41vlbd_0Kte"
      },
      "source": [
        "for j in range(len(array_text)):\n",
        "    keys = (keywords.keywords(array_text[j], words=5)).split(\"\\n\")\n",
        "    kewords_list.extend(keys)\n",
        "#         print(\"Keywords of article\", str(j+1), \"\\n\", keys)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpW70m450Ktg"
      },
      "source": [
        "# dic_key=pd.Series(kewords_list).value_counts().to_dict()\n",
        "# print(\"len of kewords_list is {} and count of unique word is {} \".format(len(kewords_list),len(dic_key)))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJNrkW40Kth"
      },
      "source": [
        "def removeWeirdChars(text):\n",
        "    weridPatterns = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u'\\U00010000-\\U0010ffff'\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\u3030\"\n",
        "                               u\"\\ufe0f\"\n",
        "                               u\"\\u2069\"\n",
        "                               u\"\\u2066\"\n",
        "                               u\"\\u200c\"\n",
        "                               u\"\\u2068\"\n",
        "                               u\"\\u2067\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return weridPatterns.sub(r'', text)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MidqRs1e0Kth"
      },
      "source": [
        "stopwords = ['n','یه', 'تو','دیگه','تورو','دارم','میشه','رو','چی','ای','کن','منو','من','بگو','اون','خوئم','واسه','تویی','وقتی','نیست','کنم','داری','آخه','منی','خودم','خودت','نمیشه','نیست','باشه','باشی','باشه','شه','میشم','ماند','','','','','','','','']\n",
        "\n",
        "def remove_general_stopwords(text):\n",
        "    filtered_tokens = [token for token in text if token not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "kewords_list = remove_general_stopwords(kewords_list)\n",
        "\n",
        "all_text =''\n",
        "for word in kewords_list:\n",
        "    all_text += word\n",
        "\n",
        "# Generate a word cloud image\n",
        "wordcloud = PersianWordCloud(\n",
        "        only_persian=True,\n",
        "        max_words=100,\n",
        "        margin=0,\n",
        "        width=800,\n",
        "        height=800,\n",
        "        min_font_size=1,\n",
        "        max_font_size=500,\n",
        "        background_color=\"white\").generate(all_text)\n",
        "image = wordcloud.to_image()\n",
        "image.save(\"res.png\")"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}